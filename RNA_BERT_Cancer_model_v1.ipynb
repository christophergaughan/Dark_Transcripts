{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1aM5bsMQ-5lF29fMHAi2D1JJ3Rzaey8bq",
      "authorship_tag": "ABX9TyNBSX3Gm3qRMtQ2iSJ72nX+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christophergaughan/Dark_Transcripts/blob/main/RNA_BERT_Cancer_model_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hypothesis\n",
        "Neural networks, particularly BERT-like models, can effectively identify patterns in RNA sequences that differentiate cancerous and non-cancerous samples. By leveraging the sequential nature of RNA and the contextual learning capabilities of BERT, we hypothesize that a model trained on labeled RNA sequences can generalize to classify new sequences as \"cancerous\" or \"non-cancerous,\" potentially uncovering biologically relevant patterns. Here we begin by exploring this hypothesis using `The Cancer Genome Atlas (TCGA)` to obtain samples of cancerous and non cancerous genes.\n",
        "\n",
        "---\n",
        "\n",
        "## Strengths of the Approach\n",
        "\n",
        "1. **Neural Networks for Pattern Detection**:\n",
        "   - BERT models excel at understanding sequential data and finding patterns within text-like data.\n",
        "   - Applying this to RNA sequences leverages the same strengths since RNA can be represented as strings of nucleotides (A, T/U, G, C).\n",
        "\n",
        "2. **Structured Supervised Learning**:\n",
        "   - Training on labeled data (\"cancerous\" vs. \"non-cancerous\") provides a clear learning objective.\n",
        "   - A properly trained model can generalize to unseen sequences, identifying cancerous patterns based on learned data.\n",
        "\n",
        "3. **Potential to Uncover Novel Patterns**:\n",
        "   - BERT may identify subtle or unknown motifs associated with cancerous transformations, aiding biological understanding.\n",
        "\n",
        "4. **Interpretability Possibilities**:\n",
        "   - Techniques like attention visualizations can help interpret which sequence regions are most predictive of cancer.\n",
        "\n",
        "---\n",
        "\n",
        "## Critiques and Challenges\n",
        "\n",
        "1. **Data Quality and Quantity**:\n",
        "   - RNA sequences are complex, and cancerous vs. non-cancerous differences might be subtle.\n",
        "   - A large, diverse, and well-labeled dataset is crucial to avoid overfitting.\n",
        "\n",
        "2. **Biological Variability**:\n",
        "   - Cancerous states may result from various factors (mutations, epigenetics, etc.) that are not always directly observable in RNA sequences.\n",
        "   - Non-cancerous samples (e.g., \"Solid Tissue Normal\") may vary widely, introducing noise.\n",
        "\n",
        "3. **Feature Representation**:\n",
        "   - RNA sequences are inherently long. Feeding raw sequences into a BERT model could result in computational inefficiency.\n",
        "   - Consider preprocessing to break sequences into biologically meaningful units (e.g., codons, motifs) or truncate/normalize sequence length.\n",
        "\n",
        "4. **Model Choice**:\n",
        "   - While BERT is powerful, DNA/RNA-specific architectures (e.g., DNABERT) may yield better results.\n",
        "   - Pretraining on a large corpus of RNA sequences before fine-tuning might improve performance.\n",
        "\n",
        "5. **Interpretability Trade-offs**:\n",
        "   - Neural networks, particularly transformers, are often seen as black boxes.\n",
        "   - For biological applications, interpretability is critical. Combining BERT with tools for explanation is important.\n",
        "\n",
        "---\n",
        "\n",
        "## Suggestions for Refinement\n",
        "\n",
        "### 1. **Preprocessing**:\n",
        "   - Ensure uniformity in RNA sequences (e.g., consistent capitalization, removal of ambiguous nucleotides like \"N\").\n",
        "   - Normalize sequences to manage varying lengths.\n",
        "\n",
        "### 2. **Experiment with Architectures**:\n",
        "   - Compare BERT models with alternatives like CNNs, RNNs, or DNA/RNA-specific models (e.g., DNABERT, ProtBERT).\n",
        "   - Explore pre-trained models for DNA/RNA sequences.\n",
        "\n",
        "### 3. **Data Augmentation**:\n",
        "   - Introduce small variations to non-cancerous sequences to simulate natural variability and improve robustness.\n",
        "\n",
        "### 4. **Benchmark Against Baseline Models**:\n",
        "   - Train simpler models (e.g., logistic regression, random forests) on extracted features to establish a baseline.\n",
        "   - Validating with baseline models ensures the data has predictive signals.\n",
        "\n",
        "### 5. **Plan for Evaluation**:\n",
        "   - Use cross-validation to ensure the model generalizes well.\n",
        "   - Consider additional metrics (e.g., precision, recall, ROC-AUC) for imbalanced datasets.\n",
        "\n",
        "### 6. **Interpretability Framework**:\n",
        "   - Use attention weights or tools like SHAP (SHapley Additive exPlanations) to interpret predictions.\n",
        "   - Highlight predictive sequence regions and validate findings with biological experts.\n",
        "\n",
        "---\n",
        "\n",
        "## Final Thoughts\n",
        "This approach is scientifically sound and forward-looking. If executed carefully, it has the potential to reveal both computational and biological insights. The challenges primarily lie in data preprocessing, architecture selection, and model interpretability, all of which can be managed with thoughtful experimentation and analysis.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "15REj4ozQBmW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZSH5CU23QA6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Anticipating Problems and Solutions**\n",
        "\n",
        "| **Potential Problems**                              | **Possible Solutions**                                                                 |\n",
        "|-----------------------------------------------------|---------------------------------------------------------------------------------------|\n",
        "| **1. Limited availability of RNA-Seq data for some cancer subtypes** | Use publicly available datasets (e.g., TCGA, GEO, GTEx) and focus on well-studied subtypes initially. Expand to other subtypes as data becomes available. |\n",
        "| **2. Imbalanced dataset (fewer cancerous genes compared to non-cancerous)** | Use techniques like oversampling cancerous data, undersampling non-cancerous data, or generating synthetic data with mutations. |\n",
        "| **3. No direct healthy counterpart for some cancerous genes** | Use genes with similar biological functions, from the same pathway, or from paralogous families as counterparts. Validate biological relevance using KEGG or Reactome. |\n",
        "| **4. Noisy or incomplete RNA-Seq data**             | Preprocess data carefully by filtering out low-quality sequences, normalizing counts, and ensuring uniform length for sequences. |\n",
        "| **5. High dimensionality of RNA sequences**         | Use k-mer tokenization to reduce dimensionality and represent sequences in a format compatible with the BERT model. |\n",
        "| **6. Difficulty in identifying biologically relevant genes** | Perform differential expression analysis with stringent criteria (e.g., p-value and fold change thresholds) and cross-reference with cancer gene databases (e.g., COSMIC, OncoKB). |\n",
        "| **7. Computational challenges with large datasets** | Use cloud computing resources (e.g., Google Colab, AWS) and optimize code to handle large-scale processing efficiently. |\n",
        "| **8. Overfitting on training data**                 | Split data into training, validation, and test sets. Use data augmentation and regularization techniques (e.g., dropout) during model training. |\n",
        "| **9. Tokenization challenges for very long RNA sequences** | Truncate or split long sequences into manageable chunks. Train the BERT model with a max sequence length suitable for the majority of the data. |\n",
        "| **10. Lack of robust evaluation metrics**           | Use a combination of metrics like accuracy, precision, recall, F1-score, and ROC-AUC to evaluate model performance comprehensively. |\n",
        "| **11. Ethical and data privacy concerns**           | Ensure that all datasets used are open-access and comply with data usage guidelines. Avoid sharing sensitive patient information. |\n",
        "| **12. Difficulties in reproducing results**         | Create a clear and modular pipeline using reproducible tools (e.g., Jupyter/Colab notebooks, version control with Git). Document all steps and dependencies. |\n",
        "\n",
        "---\n",
        "\n",
        "## **Summary**\n",
        "By anticipating these challenges and preparing solutions, we can ensure a smoother execution of the project and minimize potential setbacks.\n"
      ],
      "metadata": {
        "id": "GGfrU_YPxjMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steps to Build gdc-client in Google Colab"
      ],
      "metadata": {
        "id": "TE3dGcEx_bHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NCI-GDC/gdc-client.git\n"
      ],
      "metadata": {
        "id": "7cWZQZcBCVkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd gdc-client\n"
      ],
      "metadata": {
        "id": "g-T4gWDFCWWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "ZeITzP38CZVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd bin\n"
      ],
      "metadata": {
        "id": "YGDnEDuNCeDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x package\n"
      ],
      "metadata": {
        "id": "ei1Cec8vCicx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./package\n"
      ],
      "metadata": {
        "id": "lmHh_9D5CmP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd gdc-client/bin\n",
        "\n"
      ],
      "metadata": {
        "id": "HPw9_KVkCpco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x package\n"
      ],
      "metadata": {
        "id": "5SGPT3dQCuW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./package\n"
      ],
      "metadata": {
        "id": "uxNi4whODITp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "id": "MoBLKhvZDL1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "dDxF3TJvDZ_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install virtualenv\n"
      ],
      "metadata": {
        "id": "Cv3jd2XlDe8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd bin\n"
      ],
      "metadata": {
        "id": "uIajj9LDDxTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x package\n"
      ],
      "metadata": {
        "id": "rWsIifmQD3AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./package\n"
      ],
      "metadata": {
        "id": "uM2fWwbID6UI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/gdc-client/bin\n"
      ],
      "metadata": {
        "id": "-G2vitJlBHKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/gdc-client/bin/gdc-client download -m /path/to/new_manifest.csv -d /path/to/destination\n",
        "\n"
      ],
      "metadata": {
        "id": "hlvNgtdTEbkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/gdc-client/bin\n"
      ],
      "metadata": {
        "id": "Z2WhlpaAErXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/gdc-client/bin/gdc-client_2.3_Ubuntu_x64.zip -d /content/gdc-client/bin/\n"
      ],
      "metadata": {
        "id": "Azpl7BqJB97G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/gdc-client/bin/\n"
      ],
      "metadata": {
        "id": "fV_5_tU9CD2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x /content/gdc-client/bin/gdc-client\n"
      ],
      "metadata": {
        "id": "Y7YKmTDZCbCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/gdc-client/bin/gdc-client download -m /content/new_manifest.csv -d /content/downloaded_files/\n"
      ],
      "metadata": {
        "id": "tD1B-XubCidY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdc-client/bin/gdc-client/bin/gdc-client/bin\n"
      ],
      "metadata": {
        "id": "PCPASoArExxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Formatting\n",
        "To train a BERT-based RNA model, the dataset should be formatted as follows:\n",
        "\n",
        "**Structure of the Dataset**\n",
        "Each row should have:\n",
        "\n",
        "1. RNA Sequence: The RNA sequence (cancerous or non-cancerous).\n",
        "2. Label: A binary label indicating whether the sequence is cancerous (`1`) or non-cancerous (`0`).\n",
        "\n",
        "**Transforming the DataFrame**\n",
        "\n",
        "Assuming we have RNA sequence data files (`file_id`, `file_name`) and labels (`sample_type`), we can parse the RNA sequences from the files and structure the data:"
      ],
      "metadata": {
        "id": "q1K6ZtKGs6Gx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Paths to the uploaded files\n",
        "gdc_manifest_path = '/content/drive/MyDrive/RNA_sequence_files/gdc_manifest.csv'\n",
        "new_manifest_path = '/content/drive/MyDrive/RNA_sequence_files/new_manifest.csv'\n",
        "\n",
        "# Read the contents of the files\n",
        "try:\n",
        "    gdc_manifest = pd.read_csv(gdc_manifest_path)\n",
        "    new_manifest = pd.read_csv(new_manifest_path)\n",
        "\n",
        "    # Display the first few rows of each manifest\n",
        "    print(\"GDC Manifest (first few rows):\")\n",
        "    print(gdc_manifest.head(), \"\\n\")\n",
        "\n",
        "    print(\"New Manifest (first few rows):\")\n",
        "    print(new_manifest.head())\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error reading files: {e}\")\n"
      ],
      "metadata": {
        "id": "p0xgBt6PEcHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/gdc-client/bin/gdc-client download -m /content/drive/MyDrive/RNA_sequence_files/gdc_manifest.csv -d /content/drive/MyDrive/RNA_sequence_files/downloaded_files\n"
      ],
      "metadata": {
        "id": "WoNotdCyEdUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "-mZfq1w7FwiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "-VqIwrAgF6fW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/drive/MyDrive/RNA_sequence_files/gdc_manifest.csv /content/gdc-client/bin/\n"
      ],
      "metadata": {
        "id": "_azTJjiKGHUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "AkCjOBanGab9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/drive/MyDrive/RNA_sequence_files/new_manifest.csv /content/gdc-client/bin/\n"
      ],
      "metadata": {
        "id": "92UH2NhjGcUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "fZTm2GKpGltf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./gdc-client download -m gdc_manifest.csv -d /content/drive/MyDrive/RNA_sequence_files/downloaded_files\n"
      ],
      "metadata": {
        "id": "jOrLLJ5oGnj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert gdc_manifest.csv to gdc_manifest.txt\n",
        "gdc_manifest_path = \"/content/gdc-client/bin/gdc_manifest.csv\"\n",
        "gdc_manifest_txt_path = \"/content/gdc-client/bin/gdc_manifest.txt\"\n",
        "gdc_manifest_df = pd.read_csv(gdc_manifest_path, header=None)  # No headers in the CSV\n",
        "gdc_manifest_df.to_csv(gdc_manifest_txt_path, sep='\\t', index=False, header=['id'])\n",
        "\n",
        "# Convert new_manifest.csv to new_manifest.txt\n",
        "new_manifest_path = \"/content/gdc-client/bin/new_manifest.csv\"\n",
        "new_manifest_txt_path = \"/content/gdc-client/bin/new_manifest.txt\"\n",
        "new_manifest_df = pd.read_csv(new_manifest_path, header=None)  # No headers in the CSV\n",
        "new_manifest_df.to_csv(new_manifest_txt_path, sep='\\t', index=False, header=['id', 'filename'])\n",
        "\n",
        "print(\"Manifests converted to .txt format.\")\n"
      ],
      "metadata": {
        "id": "h7e3og4yG3Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "HmRbzOwVHPGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line is wrong because it is looking in the wrong directory, we have moved the files to:\n",
        "`/content/gdc-client/bin` this appears to have been key to this process"
      ],
      "metadata": {
        "id": "grTPV_gbKpGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./gdc-client download -m gdc_manifest.txt -d /content/drive/MyDrive/RNA_sequence_files/downloaded_files\n"
      ],
      "metadata": {
        "id": "GrwgqlXWHS-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./gdc-client download -m gdc_manifest.txt\n"
      ],
      "metadata": {
        "id": "b81NqfRjHd09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "4OJwnNX7IanX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "gUqwRnL0LbgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!file <filename>\n"
      ],
      "metadata": {
        "id": "mCcxWoA2LeDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 <filename>  # For text-based files like .tsv\n"
      ],
      "metadata": {
        "id": "B-UEnW3oMdC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!file gdc_manifest.txt\n"
      ],
      "metadata": {
        "id": "xCZ3DEyoMhKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 gdc_manifest.txt\n"
      ],
      "metadata": {
        "id": "qAPE_-7tM57r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls | grep 1d4c26b3-b9cd-4c63-9bed-84906d01ed21\n"
      ],
      "metadata": {
        "id": "4tu7CgClM_TP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "manifest = pd.read_csv('gdc_manifest.txt', sep='\\t')  # Adjust separator if needed\n",
        "print(manifest.head())\n"
      ],
      "metadata": {
        "id": "yFJRjukjNPT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### It looks like the gdc_manifest.txt file has been successfully parsed, and its contents align with the downloaded files in your directory. The file contains a single column labeled id, listing unique identifiers for the files.\n",
        "\n",
        "**Next Steps**\n",
        "* Validate All IDs: Ensure every ID in the manifest has a corresponding file in the directory:\n"
      ],
      "metadata": {
        "id": "xpHOn4c6Nol7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "manifest_ids = set(manifest['id'])\n",
        "downloaded_files = set(os.listdir('.'))  # List all files in the current directory\n",
        "\n",
        "missing_ids = manifest_ids - downloaded_files\n",
        "if missing_ids:\n",
        "    print(f\"Missing files for IDs: {missing_ids}\")\n",
        "else:\n",
        "    print(\"All files from the manifest are present.\")\n"
      ],
      "metadata": {
        "id": "QJt5JQEINZZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Map Metadata: If you have additional metadata files (e.g., `new_manifest.csv`) linking these IDs to cancerous/non-cancerous labels or other attributes, merge them with the current manifest for further analysis\n"
      ],
      "metadata": {
        "id": "KsMU3GHoN_gT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = pd.read_csv('new_manifest.csv')  # Adjust path and format if needed\n",
        "merged = manifest.merge(metadata, left_on='id', right_on='file_id', how='left')  # Use the appropriate column\n",
        "print(merged.head())\n"
      ],
      "metadata": {
        "id": "zSojq6mBN29P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the Column Names in new_manifest.csv\n",
        "Run the following code to inspect the columns in `new_manifest.csv`\n"
      ],
      "metadata": {
        "id": "6Q26II2TOeP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = pd.read_csv('new_manifest.csv')\n",
        "print(metadata.columns)\n"
      ],
      "metadata": {
        "id": "mA78TVHEOQjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjust the Merge Based on the Correct Column Names\n",
        "Once you know the correct column name for the IDs in `new_manifest.csv`, update the `merge` statement. For example:\n",
        "\n",
        "If the column for IDs in `new_manifest.csv` is named `id`, you can update the merge:\n"
      ],
      "metadata": {
        "id": "CSGMpIuEOus7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged = manifest.merge(metadata, on='id', how='left')\n"
      ],
      "metadata": {
        "id": "LEYn7MXoOn6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the CSV files lack headers, we need to explicitly set headers or handle the lack of headers during reading and merging. Here's how to proceed:\n",
        "\n",
        "#### Read the Files Without Headers\n",
        "We can specify `header=None` while reading the files to treat all rows as data\n"
      ],
      "metadata": {
        "id": "F2jTKjmAPncf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the manifest file without headers\n",
        "manifest = pd.read_csv('gdc_manifest.txt', sep='\\t', header=None, names=['id'])\n",
        "\n",
        "# Read the metadata file without headers\n",
        "metadata = pd.read_csv('new_manifest.csv', header=None, names=['file_id', 'file_name'])\n",
        "\n",
        "# Inspect the data\n",
        "print(manifest.head())\n",
        "print(metadata.head())\n"
      ],
      "metadata": {
        "id": "vCabvEbqPGIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the header-like row from the manifest DataFrame\n",
        "manifest = manifest[manifest['id'] != 'id']\n",
        "\n",
        "# Merge on the correct columns\n",
        "merged = manifest.merge(metadata, left_on='id', right_on='file_id', how='left')\n",
        "\n",
        "# Inspect the merged DataFrame\n",
        "print(merged.head())\n"
      ],
      "metadata": {
        "id": "u8ajBtb8P4mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `merged` DataFrame successfully aligns the IDs from the `manifest` with the corresponding files from the `metadata`. Here's what the result indicates:\n",
        "\n",
        "#### Structure of the `merged` DataFrame:\n",
        "* Columns:\n",
        "    * `id`: The IDs from the `manifest`.\n",
        "    * `file_id`: Corresponding IDs from the `metadata`, verifying the match.\n",
        "    * `file_name`: The RNA sequence file names associated with the IDs.\n",
        "### Next Steps:\n",
        "1. Validate Data Completeness:\n",
        "\n",
        "Check if any IDs in the manifest were not matched in the metadata.\n",
        "Use merged to identify rows where file_name is NaN."
      ],
      "metadata": {
        "id": "ur2yYl5qQmIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_files = merged[merged['file_name'].isnull()]\n",
        "print(f\"Number of missing files: {missing_files.shape[0]}\")\n",
        "print(missing_files)\n"
      ],
      "metadata": {
        "id": "baDaUJxTQWKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Filter by Desired Criteria:\n",
        "\n",
        "    * Extract specific subsets of data, e.g., cancerous vs. non-cancerous samples, if such labels exist in metadata.\n",
        "    * If no labels are present, we may need to cross-reference another dataset for annotations.\n",
        "3. Prepare for Sequence Processing:\n",
        "\n",
        "    * Confirm the physical existence of the files listed in file_name.\n",
        "    * Load these files to inspect their contents and validate RNA sequence formats.\n",
        "4. Save the Processed Data:\n",
        "\n",
        "    * Save the merged DataFrame as a CSV file for future reference"
      ],
      "metadata": {
        "id": "hC8msZVbRfCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged.to_csv('merged_manifest.csv', index=False)\n",
        "print(\"Merged manifest saved to 'merged_manifest.csv'\")\n"
      ],
      "metadata": {
        "id": "o3d28QTBRUGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la"
      ],
      "metadata": {
        "id": "e1zIDYXRSAVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Verify Physical Files:\n",
        "\n",
        "    * Ensure all files listed in file_name are present in the directory where the files were downloaded."
      ],
      "metadata": {
        "id": "YQqzIGKDSZ6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Directory where files are downloaded\n",
        "download_dir = '/content/gdc-client/bin'\n",
        "\n",
        "# List of expected files\n",
        "expected_files = set(merged['file_name'])\n",
        "\n",
        "# Files present in the directory\n",
        "downloaded_files = set(os.listdir(download_dir))\n",
        "\n",
        "# Missing files\n",
        "missing_files = expected_files - downloaded_files\n",
        "if missing_files:\n",
        "    print(f\"Missing files: {len(missing_files)}\")\n",
        "    print(missing_files)\n",
        "else:\n",
        "    print(\"All files are present.\")\n"
      ],
      "metadata": {
        "id": "XdTzFEwBSTds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "merged_manifest = pd.read_csv('merged_manifest.csv')\n",
        "print(merged_manifest.head())\n"
      ],
      "metadata": {
        "id": "ACW6flaTSnWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "download_dir = '/content/gdc-client/bin'  # Adjust if needed\n",
        "downloaded_files = [f for f in os.listdir(download_dir) if f.endswith('.tsv')]\n",
        "print(f\"Downloaded files count: {len(downloaded_files)}\")\n"
      ],
      "metadata": {
        "id": "8kkhIWj9TEZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_manifest = merged_manifest[~merged_manifest['file_name'].isin(downloaded_files)]\n",
        "missing_manifest[['id']].to_csv('missing_manifest.txt', index=False, header=False)\n"
      ],
      "metadata": {
        "id": "5ycgGeIOTRZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "downloaded_files = set(f.rsplit('.', 1)[0] for f in os.listdir(download_dir))\n"
      ],
      "metadata": {
        "id": "GW1BbYE9TZLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('missing_manifest.txt', 'r') as f:\n",
        "    print(f.readlines()[:10])  # Print first 10 lines for verification\n"
      ],
      "metadata": {
        "id": "0ZoJ_pX_UBGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls -l missing_manifest.txt\n"
      ],
      "metadata": {
        "id": "UmJDhDG-UESo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls -ld /content/gdc-client/bin\n"
      ],
      "metadata": {
        "id": "WKQSsMGYUXiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./gdc-client download -m missing_manifest.txt -d /content/gdc-client/bin\n"
      ],
      "metadata": {
        "id": "dDLRnA8LUiwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "p2hbGfelU2pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 10 missing_manifest.txt\n"
      ],
      "metadata": {
        "id": "rdPLTDo0VEzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat -A missing_manifest.txt | head -n 10\n"
      ],
      "metadata": {
        "id": "ACBq4viLVWtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!file missing_manifest.txt\n"
      ],
      "metadata": {
        "id": "FOCppCSOVgGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/[[:space:]]*$//' missing_manifest.txt\n"
      ],
      "metadata": {
        "id": "gyQOKty6Vo8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!dos2unix missing_manifest.txt\n"
      ],
      "metadata": {
        "id": "0FIaIJFvV_l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat -A missing_manifest.txt | head -n 10\n"
      ],
      "metadata": {
        "id": "f1GlQCuxWD-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/\\r$//' missing_manifest.txt\n"
      ],
      "metadata": {
        "id": "6HIK6ypgWKBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat -A missing_manifest.txt | head -n 10\n"
      ],
      "metadata": {
        "id": "L7hfBTpGWa4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('missing_manifest.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Remove any carriage return characters\n",
        "lines = [line.strip() + '\\n' for line in lines]\n",
        "\n",
        "with open('missing_manifest.txt', 'w') as file:\n",
        "    file.writelines(lines)\n"
      ],
      "metadata": {
        "id": "L93NiM_FWdb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat -A missing_manifest.txt | head -n 10\n"
      ],
      "metadata": {
        "id": "Rr78jL0fWz0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!xxd missing_manifest.txt | head -n 10\n"
      ],
      "metadata": {
        "id": "9mwRH9TIW3H8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 10 missing_manifest.txt\n"
      ],
      "metadata": {
        "id": "UWgO97O3XcE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./gdc-client download -m missing_manifest.txt -d /content/gdc-client/bin\n"
      ],
      "metadata": {
        "id": "bIwo773DXlz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `ERROR: Invalid manifest` issue persists, suggesting there is still some problem with the missing_manifest.txt file format or structure. Let's take another systematic approach to resolve this:\n",
        "\n",
        "#### Troubleshooting Steps:\n",
        "1. Manifest File Format\n",
        "\n",
        "    * Ensure the file starts without a header. The `missing_manifest.txt` file should only contain the file IDs (no column names or extra lines).\n",
        "    * Double-check that each line contains a valid UUID (no extra spaces, symbols, or characters).\n",
        "2. File Encoding\n",
        "\n",
        "    * Verify the encoding of the file to ensure it is in plain `ASCII` or `UTF-8` without a `BOM` (Byte Order Mark). This can be done using the file command:\n",
        "\n"
      ],
      "metadata": {
        "id": "8SZXd_8lYSc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!file missing_manifest.txt\n"
      ],
      "metadata": {
        "id": "4Yz1rFycX_HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat -A missing_manifest.txt | head -n 10\n"
      ],
      "metadata": {
        "id": "hAGEBEs6Y-7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!diff missing_manifest.txt sample_manifest.txt\n"
      ],
      "metadata": {
        "id": "ZGBeArSJZHUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la"
      ],
      "metadata": {
        "id": "nI30CvsnZS3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tr -d '\\r' < missing_manifest.txt > sanitized_manifest.txt\n"
      ],
      "metadata": {
        "id": "pPcQbnhyZgAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv sanitized_manifest.txt missing_manifest.txt\n"
      ],
      "metadata": {
        "id": "XnUaDlWKZ6XW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!file missing_manifest.txt\n"
      ],
      "metadata": {
        "id": "Y4xfJzmYaBAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "with open(\"missing_manifest.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "uuid_pattern = re.compile(r'^[a-f0-9\\-]{36}$')\n",
        "valid = all(uuid_pattern.match(line.strip()) for line in lines)\n",
        "\n",
        "print(\"All UUIDs valid:\", valid)\n"
      ],
      "metadata": {
        "id": "6jtMLWmsaH4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x ./gdc-client\n"
      ],
      "metadata": {
        "id": "y4e8xT56aV6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./gdc-client download -m missing_manifest.txt -d /content/gdc-client/bin\n"
      ],
      "metadata": {
        "id": "oCo_oMAUarZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r session_backup.zip /content/\n"
      ],
      "metadata": {
        "id": "bqj_65W3a0bE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl 'https://api.gdc.cancer.gov/files/183f73e2-9ddd-4fee-aab3-1b850855a0fd?pretty=true'\n"
      ],
      "metadata": {
        "id": "MSLvnZD4bl8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl 'https://api.gdc.cancer.gov/files/00252d7f-e222-462f-badc-b97e8dce2021?pretty=true'\n"
      ],
      "metadata": {
        "id": "tnpmWjfWw8hL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl 'https://api.gdc.cancer.gov/files/_mapping?pretty=true'\n"
      ],
      "metadata": {
        "id": "MU3w-ePhxtqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl 'https://api.gdc.cancer.gov/cases/_mapping?pretty=true'\n"
      ],
      "metadata": {
        "id": "0VVUNPdlySfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl 'https://api.gdc.cancer.gov/files/183f73e2-9ddd-4fee-aab3-1b850855a0fd?fields=cases.samples.sample_type,cases.samples.submitter_id&pretty=true'\n"
      ],
      "metadata": {
        "id": "0aBmNga_ycr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl 'https://api.gdc.cancer.gov/files?filters={\"op\":\"in\",\"content\":{\"field\":\"files.file_id\",\"value\":[\"183f73e2-9ddd-4fee-aab3-1b850855a0fd\",\"<another_uuid>\"]}}&fields=cases.samples.sample_type,cases.samples.submitter_id&pretty=true'\n"
      ],
      "metadata": {
        "id": "q_Sita4syoIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl 'https://api.gdc.cancer.gov/files?filters={\"op\":\"and\",\"content\":[{\"op\":\"in\",\"content\":{\"field\":\"cases.samples.sample_type\",\"value\":[\"Primary Tumor\",\"Metastatic\"]}}]}&fields=cases.samples.sample_type,cases.samples.submitter_id&pretty=true'\n"
      ],
      "metadata": {
        "id": "RBQ62Walz-M1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# GDC API endpoint\n",
        "BASE_URL = \"https://api.gdc.cancer.gov/files\"\n",
        "\n",
        "def query_sample_type(uuids):\n",
        "    \"\"\"\n",
        "    Query the GDC API for sample types for a given list of UUIDs.\n",
        "    \"\"\"\n",
        "    # Prepare filters JSON\n",
        "    filters = {\n",
        "        \"op\": \"in\",\n",
        "        \"content\": {\n",
        "            \"field\": \"files.file_id\",\n",
        "            \"value\": uuids\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # API request payload\n",
        "    payload = {\n",
        "        \"filters\": filters,\n",
        "        \"fields\": \"cases.samples.sample_type,cases.samples.submitter_id\",\n",
        "        \"format\": \"JSON\",\n",
        "        \"size\": len(uuids)\n",
        "    }\n",
        "\n",
        "    # Make POST request\n",
        "    response = requests.post(BASE_URL, json=payload)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}, {response.text}\")\n",
        "        return None\n",
        "\n",
        "def categorize_samples(api_response):\n",
        "    \"\"\"\n",
        "    Categorize samples into Cancerous and Non-Cancerous.\n",
        "    \"\"\"\n",
        "    cancerous = []\n",
        "    non_cancerous = []\n",
        "\n",
        "    for file in api_response.get('data', {}).get('hits', []):\n",
        "        for sample in file.get('cases', [{}])[0].get('samples', []):\n",
        "            sample_type = sample.get('sample_type')\n",
        "            submitter_id = sample.get('submitter_id')\n",
        "\n",
        "            if sample_type in [\"Primary Tumor\", \"Metastatic\"]:\n",
        "                cancerous.append({\"submitter_id\": submitter_id, \"type\": sample_type})\n",
        "            elif sample_type in [\"Solid Tissue Normal\", \"Blood Derived Normal\"]:\n",
        "                non_cancerous.append({\"submitter_id\": submitter_id, \"type\": sample_type})\n",
        "\n",
        "    return cancerous, non_cancerous\n",
        "\n",
        "# Example usage\n",
        "uuid_list = [\"183f73e2-9ddd-4fee-aab3-1b850855a0fd\", \"another_uuid_here\"]\n",
        "response = query_sample_type(uuid_list)\n",
        "\n",
        "if response:\n",
        "    cancerous, non_cancerous = categorize_samples(response)\n",
        "    print(\"Cancerous Samples:\", cancerous)\n",
        "    print(\"Non-Cancerous Samples:\", non_cancerous)\n"
      ],
      "metadata": {
        "id": "S5IJb0NN0JOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# File containing UUIDs\n",
        "UUID_FILE = \"missing_manifest.txt\"\n",
        "\n",
        "# GDC API endpoint\n",
        "BASE_URL = \"https://api.gdc.cancer.gov/files\"\n",
        "\n",
        "def load_uuids(file_path):\n",
        "    \"\"\"\n",
        "    Load UUIDs from a file into a list.\n",
        "    \"\"\"\n",
        "    with open(file_path, \"r\") as file:\n",
        "        return [line.strip() for line in file.readlines() if line.strip()]\n",
        "\n",
        "def query_sample_type(uuids):\n",
        "    \"\"\"\n",
        "    Query the GDC API for sample types for a given list of UUIDs.\n",
        "    \"\"\"\n",
        "    # Prepare filters JSON\n",
        "    filters = {\n",
        "        \"op\": \"in\",\n",
        "        \"content\": {\n",
        "            \"field\": \"files.file_id\",\n",
        "            \"value\": uuids\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # API request payload\n",
        "    payload = {\n",
        "        \"filters\": filters,\n",
        "        \"fields\": \"cases.samples.sample_type,cases.samples.submitter_id\",\n",
        "        \"format\": \"JSON\",\n",
        "        \"size\": len(uuids)\n",
        "    }\n",
        "\n",
        "    # Make POST request\n",
        "    response = requests.post(BASE_URL, json=payload)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}, {response.text}\")\n",
        "        return None\n",
        "\n",
        "def categorize_samples(api_response):\n",
        "    \"\"\"\n",
        "    Categorize samples into Cancerous and Non-Cancerous.\n",
        "    \"\"\"\n",
        "    cancerous = []\n",
        "    non_cancerous = []\n",
        "\n",
        "    for file in api_response.get('data', {}).get('hits', []):\n",
        "        for sample in file.get('cases', [{}])[0].get('samples', []):\n",
        "            sample_type = sample.get('sample_type')\n",
        "            submitter_id = sample.get('submitter_id')\n",
        "\n",
        "            if sample_type in [\"Primary Tumor\", \"Metastatic\"]:\n",
        "                cancerous.append({\"submitter_id\": submitter_id, \"type\": sample_type})\n",
        "            elif sample_type in [\"Solid Tissue Normal\", \"Blood Derived Normal\"]:\n",
        "                non_cancerous.append({\"submitter_id\": submitter_id, \"type\": sample_type})\n",
        "\n",
        "    return cancerous, non_cancerous\n",
        "\n",
        "def batch_process_uuids(uuid_list, batch_size=100):\n",
        "    \"\"\"\n",
        "    Process UUIDs in batches to query the API.\n",
        "    \"\"\"\n",
        "    cancerous_all = []\n",
        "    non_cancerous_all = []\n",
        "\n",
        "    for i in range(0, len(uuid_list), batch_size):\n",
        "        batch = uuid_list[i:i + batch_size]\n",
        "        print(f\"Processing batch {i // batch_size + 1}...\")\n",
        "        response = query_sample_type(batch)\n",
        "        if response:\n",
        "            cancerous, non_cancerous = categorize_samples(response)\n",
        "            cancerous_all.extend(cancerous)\n",
        "            non_cancerous_all.extend(non_cancerous)\n",
        "\n",
        "    return cancerous_all, non_cancerous_all\n",
        "\n",
        "# Main logic\n",
        "if __name__ == \"__main__\":\n",
        "    uuids = load_uuids(UUID_FILE)\n",
        "    cancerous, non_cancerous = batch_process_uuids(uuids, batch_size=50)\n",
        "\n",
        "    print(\"Cancerous Samples:\", cancerous)\n",
        "    print(\"Non-Cancerous Samples:\", non_cancerous)\n"
      ],
      "metadata": {
        "id": "GAvS_cf802tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def save_to_csv(file_name, data, fieldnames):\n",
        "    \"\"\"\n",
        "    Save data to a CSV file.\n",
        "    \"\"\"\n",
        "    with open(file_name, mode=\"w\", newline=\"\") as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n",
        "\n",
        "# Save cancerous samples\n",
        "save_to_csv(\"cancerous_samples.csv\", cancerous, [\"submitter_id\", \"type\"])\n",
        "\n",
        "# Save non-cancerous samples\n",
        "save_to_csv(\"non_cancerous_samples.csv\", non_cancerous, [\"submitter_id\", \"type\"])\n",
        "\n",
        "print(\"Files saved: cancerous_samples.csv, non_cancerous_samples.csv\")\n"
      ],
      "metadata": {
        "id": "v7SREuRI1mtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl 'https://api.gdc.cancer.gov/history/00252d7f-e222-462f-badc-b97e8dce2021'\n"
      ],
      "metadata": {
        "id": "v7xRDsY518gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl 'https://api.gdc.cancer.gov/files?filters={\"op\":\"=\",\"content\":{\"field\":\"files.data_type\",\"value\":\"Gene Expression Quantification\"}}&fields=file_id,00252d7f-e222-462f-badc-b97e8dce2021,analysis.workflow_type&pretty=true'\n"
      ],
      "metadata": {
        "id": "OeACtN-i-s4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl --request POST --header \"Content-Type: application/json\" --data '\n",
        "{\n",
        "  \"filters\": {\n",
        "    \"op\": \"and\",\n",
        "    \"content\": [\n",
        "      {\n",
        "        \"op\": \"in\",\n",
        "        \"content\": {\n",
        "          \"field\": \"cases.submitter_id\",\n",
        "          \"value\": [\"SC108221_merged\", \"SC080780_merged\"]  # Replace with your sample IDs\n",
        "        }\n",
        "      },\n",
        "      {\n",
        "        \"op\": \"=\",\n",
        "        \"content\": {\n",
        "          \"field\": \"files.data_type\",\n",
        "          \"value\": \"Gene Expression Quantification\"\n",
        "        }\n",
        "      }\n",
        "    ]\n",
        "  },\n",
        "  \"fields\": \"file_id,file_name,cases.submitter_id,data_category,data_type,platform,experimental_strategy\",\n",
        "  \"format\": \"JSON\",\n",
        "  \"size\": \"1000\"\n",
        "}' 'https://api.gdc.cancer.gov/files'\n"
      ],
      "metadata": {
        "id": "7AbD7obS_PBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl --request POST --header \"Content-Type: application/json\" --data '\n",
        "{\n",
        "  \"filters\": {\n",
        "    \"op\": \"and\",\n",
        "    \"content\": [\n",
        "      {\n",
        "        \"op\": \"=\",\n",
        "        \"content\": {\n",
        "          \"field\": \"files.data_type\",\n",
        "          \"value\": \"Gene Expression Quantification\"\n",
        "        }\n",
        "      },\n",
        "      {\n",
        "        \"op\": \"=\",\n",
        "        \"content\": {\n",
        "          \"field\": \"files.experimental_strategy\",\n",
        "          \"value\": \"RNA-Seq\"\n",
        "        }\n",
        "      }\n",
        "    ]\n",
        "  },\n",
        "  \"fields\": \"file_id,file_name,cases.submitter_id,data_category,data_type,platform,experimental_strategy\",\n",
        "  \"format\": \"JSON\",\n",
        "  \"size\": \"1000\"\n",
        "}' 'https://api.gdc.cancer.gov/files'\n"
      ],
      "metadata": {
        "id": "ABU_R5aqetCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "OaEzc7XHe9KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# List all files in the directory\n",
        "directory_path = '.'  # Adjust path as needed\n",
        "all_files = os.listdir(directory_path)\n",
        "\n",
        "# Filter UUIDs (assumes UUIDs are filenames without extensions)\n",
        "uuids = [f.split('.')[0] for f in all_files if len(f.split('.')[0]) == 36]\n",
        "\n",
        "print(f\"Total UUIDs found: {len(uuids)}\")\n"
      ],
      "metadata": {
        "id": "vUyn6tRmfwxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GDC API URL\n",
        "api_url = \"https://api.gdc.cancer.gov/files\"\n",
        "headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "# Function to query API in batches\n",
        "def query_gdc(uuids, batch_size=100):\n",
        "    results = []\n",
        "    for i in range(0, len(uuids), batch_size):\n",
        "        batch = uuids[i:i + batch_size]\n",
        "        payload = {\n",
        "            \"filters\": {\n",
        "                \"op\": \"in\",\n",
        "                \"content\": {\n",
        "                    \"field\": \"files.file_id\",\n",
        "                    \"value\": batch\n",
        "                }\n",
        "            },\n",
        "            \"fields\": \"file_id,file_name,cases.submitter_id,data_category,data_type,platform\",\n",
        "            \"format\": \"JSON\",\n",
        "            \"size\": batch_size\n",
        "        }\n",
        "\n",
        "        response = requests.post(api_url, headers=headers, data=json.dumps(payload))\n",
        "        if response.status_code == 200:\n",
        "            results.extend(response.json().get('data', {}).get('hits', []))\n",
        "        else:\n",
        "            print(f\"Error: {response.status_code}, {response.text}\")\n",
        "    return results\n",
        "\n",
        "# Query the API\n",
        "rna_files = query_gdc(uuids)\n",
        "\n",
        "# Convert to DataFrame for easy manipulation\n",
        "rna_files_df = pd.DataFrame(rna_files)\n",
        "rna_files_df.to_csv(\"rna_sequence_files.csv\", index=False)\n",
        "print(\"RNA sequence files saved to rna_sequence_files.csv\")\n"
      ],
      "metadata": {
        "id": "S4QH5UyogVK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load manifest UUIDs\n",
        "manifest = pd.read_csv(\"gdc_manifest.csv\")\n",
        "uuids = manifest['id'].tolist()  # Assuming 'id' column contains UUIDs\n"
      ],
      "metadata": {
        "id": "mKEBBfAugm9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load manifest and inspect columns\n",
        "manifest = pd.read_csv(\"gdc_manifest.csv\")\n",
        "print(manifest.columns)  # Check available columns\n",
        "\n",
        "# Assuming the column with UUIDs is named 'uuid' (adjust based on output)\n",
        "uuids = manifest['uuid'].tolist()  # Replace 'uuid' with the actual column name\n",
        "print(f\"Total UUIDs found in manifest: {len(uuids)}\")\n"
      ],
      "metadata": {
        "id": "ZVqfM2cbgxdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "manifest = pd.read_csv(\"gdc_manifest.csv\")\n",
        "print(manifest.head())  # Display the first few rows of the manifest\n"
      ],
      "metadata": {
        "id": "qCGWdYiciaYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Columns:\", manifest.columns)\n",
        "print(\"Shape:\", manifest.shape)\n"
      ],
      "metadata": {
        "id": "gXqHPml9i04K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload manifest with no header\n",
        "manifest = pd.read_csv(\"gdc_manifest.csv\", header=None)\n",
        "\n",
        "# Assign a default column name\n",
        "manifest.columns = ['uuid']\n",
        "\n",
        "# Extract UUIDs as a list\n",
        "uuids = manifest['uuid'].tolist()\n",
        "\n",
        "print(f\"Total UUIDs found in manifest: {len(uuids)}\")\n"
      ],
      "metadata": {
        "id": "mSuzpNoqjAtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query the GDC API in batches\n",
        "def query_gdc(uuids, batch_size=100):\n",
        "    results = []\n",
        "    for i in range(0, len(uuids), batch_size):\n",
        "        batch = uuids[i:i + batch_size]\n",
        "        payload = {\n",
        "            \"filters\": {\n",
        "                \"op\": \"in\",\n",
        "                \"content\": {\n",
        "                    \"field\": \"files.file_id\",\n",
        "                    \"value\": batch\n",
        "                }\n",
        "            },\n",
        "            \"fields\": \"file_id,file_name,cases.submitter_id,data_category,data_type,platform\",\n",
        "            \"format\": \"JSON\",\n",
        "            \"size\": batch_size\n",
        "        }\n",
        "\n",
        "        response = requests.post(\"https://api.gdc.cancer.gov/files\", headers={\"Content-Type\": \"application/json\"}, data=json.dumps(payload))\n",
        "        if response.status_code == 200:\n",
        "            results.extend(response.json().get('data', {}).get('hits', []))\n",
        "        else:\n",
        "            print(f\"Error: {response.status_code}, {response.text}\")\n",
        "    return results\n",
        "\n",
        "# Call the function and save the results\n",
        "rna_files = query_gdc(uuids)\n",
        "rna_files_df = pd.DataFrame(rna_files)\n",
        "rna_files_df.to_csv(\"rna_sequence_files.csv\", index=False)\n",
        "print(\"RNA sequence files saved to rna_sequence_files.csv\")\n"
      ],
      "metadata": {
        "id": "UzuYZYh6jPxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the saved RNA sequence files metadata\n",
        "rna_sequence_files_path = \"rna_sequence_files.csv\"\n",
        "rna_sequence_files_df = pd.read_csv(rna_sequence_files_path)\n",
        "\n",
        "# Display the first few rows of the dataframe to verify contents\n",
        "rna_sequence_files_df.head()\n"
      ],
      "metadata": {
        "id": "7Xet8qdOjmjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load cancerous and non-cancerous datasets\n",
        "cancerous_samples = pd.read_csv(\"cancerous_samples.csv\")\n",
        "non_cancerous_samples = pd.read_csv(\"non_cancerous_samples.csv\")\n",
        "\n",
        "# Add a classification column to each dataset\n",
        "cancerous_samples['classification'] = 'cancerous'\n",
        "non_cancerous_samples['classification'] = 'non-cancerous'\n",
        "\n",
        "# Combine the two datasets\n",
        "classification_data = pd.concat([cancerous_samples, non_cancerous_samples], ignore_index=True)\n",
        "\n",
        "# Merge with RNA sequence metadata using 'submitter_id'\n",
        "rna_sequence_files_df['submitter_id'] = rna_sequence_files_df['cases'].apply(lambda x: eval(x)[0]['submitter_id'])  # Extract submitter_id from cases\n",
        "linked_data = rna_sequence_files_df.merge(classification_data, on='submitter_id', how='left')\n",
        "\n",
        "# Save and display the linked dataset\n",
        "linked_data.to_csv(\"linked_rna_sequence_files.csv\", index=False)\n",
        "print(\"Linked data saved to linked_rna_sequence_files.csv\")\n",
        "linked_data.head()\n"
      ],
      "metadata": {
        "id": "n7-_yJfXkMWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique submitter IDs in each dataset\n",
        "rna_submitter_ids = set(rna_sequence_files_df['submitter_id'])\n",
        "classification_submitter_ids = set(classification_data['submitter_id'])\n",
        "\n",
        "# Find unmatched IDs\n",
        "unmatched_ids = rna_submitter_ids - classification_submitter_ids\n",
        "print(f\"Unmatched submitter IDs: {unmatched_ids}\")\n",
        "print(f\"Total unmatched IDs: {len(unmatched_ids)}\")\n"
      ],
      "metadata": {
        "id": "ZXdMZ4JEk_WI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize case for submitter IDs\n",
        "rna_sequence_files_df['submitter_id'] = rna_sequence_files_df['submitter_id'].str.upper()\n",
        "classification_data['submitter_id'] = classification_data['submitter_id'].str.upper()\n",
        "\n",
        "# Retry merge\n",
        "linked_data = rna_sequence_files_df.merge(classification_data, on='submitter_id', how='left')\n",
        "\n",
        "# Save and inspect the results\n",
        "linked_data.to_csv(\"linked_rna_sequence_files_updated.csv\", index=False)\n",
        "print(\"Updated linked data saved to linked_rna_sequence_files_updated.csv\")\n",
        "linked_data.head()\n"
      ],
      "metadata": {
        "id": "Tw5v413Cldf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine cancerous and non-cancerous datasets\n",
        "classification_data = pd.DataFrame(cancerous + non_cancerous)\n",
        "\n",
        "# Merge the RNA sequence metadata with classification data\n",
        "linked_data = rna_sequence_files_df.merge(classification_data, on=\"submitter_id\", how=\"left\")\n",
        "\n",
        "# Save the enriched dataset\n",
        "linked_data.to_csv(\"linked_rna_sequence_classification.csv\", index=False)\n",
        "print(\"Enriched dataset saved to linked_rna_sequence_classification.csv\")\n",
        "\n",
        "# Display the first few rows for verification\n",
        "print(linked_data.head())\n"
      ],
      "metadata": {
        "id": "7Z5v-5z-lpDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unmatched_ids = linked_data[linked_data['type'].isna()]['submitter_id'].unique()\n",
        "print(f\"Unmatched Submitter IDs: {unmatched_ids}\")\n",
        "print(f\"Total Unmatched Submitter IDs: {len(unmatched_ids)}\")\n"
      ],
      "metadata": {
        "id": "Ghhalo28mh5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linked_data['submitter_id_normalized'] = linked_data['submitter_id'].str.split('-').str[:3].str.join('-')\n",
        "classification_data['submitter_id_normalized'] = classification_data['submitter_id'].str.split('-').str[:3].str.join('-')\n",
        "\n",
        "# Retry merge with normalized IDs\n",
        "linked_data = linked_data.merge(\n",
        "    classification_data[['submitter_id_normalized', 'type', 'classification']],\n",
        "    on='submitter_id_normalized',\n",
        "    how='left'\n",
        ")\n"
      ],
      "metadata": {
        "id": "lgwd_oaZm8o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_data.columns)\n"
      ],
      "metadata": {
        "id": "ufhPNfGrnGlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'classification' not in classification_data.columns:\n",
        "    classification_data['classification'] = classification_data['type'].apply(\n",
        "        lambda x: 'cancerous' if x in ['Primary Tumor', 'Metastatic'] else 'non-cancerous'\n",
        "    )\n"
      ],
      "metadata": {
        "id": "JKjyD0z6nYSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linked_data['submitter_id_normalized'] = linked_data['submitter_id'].str.split('-').str[:3].str.join('-')\n",
        "classification_data['submitter_id_normalized'] = classification_data['submitter_id'].str.split('-').str[:3].str.join('-')\n"
      ],
      "metadata": {
        "id": "HhqWMHjGnf78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linked_data = linked_data.merge(\n",
        "    classification_data[['submitter_id_normalized', 'type', 'classification']],\n",
        "    on='submitter_id_normalized',\n",
        "    how='left'\n",
        ")\n"
      ],
      "metadata": {
        "id": "VxsAUQIJnjX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(linked_data[['submitter_id', 'type', 'classification']].head())\n",
        "linked_data.to_csv(\"linked_rna_sequence_classification_updated.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "PLiTfnuxnmtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(linked_data.columns)\n"
      ],
      "metadata": {
        "id": "1eh7apnJnpqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the 'classification' column if missing in classification_data\n",
        "if 'classification' not in classification_data.columns:\n",
        "    classification_data['classification'] = classification_data['type'].apply(\n",
        "        lambda x: 'cancerous' if x in ['Primary Tumor', 'Metastatic'] else 'non-cancerous'\n",
        "    )\n",
        "\n",
        "# Retry merging with the normalized IDs\n",
        "linked_data = linked_data.merge(\n",
        "    classification_data[['submitter_id_normalized', 'type', 'classification']],\n",
        "    on='submitter_id_normalized',\n",
        "    how='left'\n",
        ")\n"
      ],
      "metadata": {
        "id": "wSDyX9ern71Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(linked_data[['submitter_id', 'type', 'classification']].head())\n"
      ],
      "metadata": {
        "id": "nj2NDEoWoDld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linked_data.to_csv(\"linked_rna_sequence_classification_updated.csv\", index=False)\n",
        "print(\"Updated data saved successfully!\")\n"
      ],
      "metadata": {
        "id": "5_aTCvfRoIiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(linked_data.columns)\n"
      ],
      "metadata": {
        "id": "MgAex_CIoSV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the 'classification' column if missing in classification_data\n",
        "if 'classification' not in classification_data.columns:\n",
        "    classification_data['classification'] = classification_data['type'].apply(\n",
        "        lambda x: 'cancerous' if x in ['Primary Tumor', 'Metastatic'] else 'non-cancerous'\n",
        "    )\n",
        "\n",
        "# Retry merging with the normalized IDs\n",
        "linked_data = linked_data.merge(\n",
        "    classification_data[['submitter_id_normalized', 'type', 'classification']],\n",
        "    on='submitter_id_normalized',\n",
        "    how='left'\n",
        ")\n"
      ],
      "metadata": {
        "id": "Zo6GtROwob9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(linked_data[['submitter_id', 'type', 'classification']].head())\n"
      ],
      "metadata": {
        "id": "5n_e18frol0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop redundant columns before merge to avoid conflicts\n",
        "columns_to_drop = ['type_x', 'type_y', 'classification_x', 'classification_y']\n",
        "linked_data = linked_data.drop(columns=[col for col in columns_to_drop if col in linked_data.columns])\n",
        "\n",
        "# Add 'classification' if missing in classification_data\n",
        "if 'classification' not in classification_data.columns:\n",
        "    classification_data['classification'] = classification_data['type'].apply(\n",
        "        lambda x: 'cancerous' if x in ['Primary Tumor', 'Metastatic'] else 'non-cancerous'\n",
        "    )\n",
        "\n",
        "# Retry merging with normalized IDs\n",
        "linked_data = linked_data.merge(\n",
        "    classification_data[['submitter_id_normalized', 'type', 'classification']],\n",
        "    on='submitter_id_normalized',\n",
        "    how='left',\n",
        "    suffixes=('', '_new')  # Avoid conflicts by using unique suffixes\n",
        ")\n",
        "\n",
        "# Display the desired columns\n",
        "print(linked_data[['submitter_id', 'type', 'classification']].head())\n",
        "\n",
        "# Save the updated dataset\n",
        "linked_data.to_csv(\"linked_rna_sequence_classification_updated.csv\", index=False)\n",
        "print(\"Updated data saved successfully!\")\n"
      ],
      "metadata": {
        "id": "J9eZORtYotQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove exact duplicates\n",
        "linked_data = linked_data.drop_duplicates()\n",
        "\n",
        "# Resolve conflicting classifications by prioritizing 'cancerous'\n",
        "def resolve_classification(group):\n",
        "    if 'cancerous' in group['classification'].values:\n",
        "        return 'cancerous'\n",
        "    return 'non-cancerous'\n",
        "\n",
        "# Aggregate classifications\n",
        "resolved_classifications = (\n",
        "    linked_data.groupby('submitter_id')['classification']\n",
        "    .apply(resolve_classification)\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Merge resolved classifications back into the dataset\n",
        "linked_data = linked_data.drop(columns=['classification']).merge(\n",
        "    resolved_classifications, on='submitter_id', how='left'\n",
        ")\n",
        "\n",
        "# Drop any remaining duplicates\n",
        "linked_data = linked_data.drop_duplicates()\n",
        "\n",
        "# Display the cleaned data\n",
        "print(linked_data[['submitter_id', 'type_x', 'classification']].head())\n",
        "\n",
        "# Save the updated dataset\n",
        "linked_data.to_csv(\"linked_rna_sequence_classification_cleaned.csv\", index=False)\n",
        "print(\"Cleaned data saved successfully!\")\n"
      ],
      "metadata": {
        "id": "StnEV5ejqImR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect columns in linked_data\n",
        "print(f\"Columns in linked_data: {linked_data.columns}\")\n",
        "\n",
        "# Verify the available type columns and classification column\n",
        "type_column = 'type_new' if 'type_new' in linked_data.columns else 'type'\n",
        "classification_column = 'classification' if 'classification' in linked_data.columns else None\n",
        "\n",
        "print(f\"Using type column: {type_column}\")\n",
        "print(f\"Using classification column: {classification_column}\")\n",
        "\n",
        "# Check and clean classification data\n",
        "if classification_column:\n",
        "    # Remove duplicates\n",
        "    linked_data = linked_data.drop_duplicates()\n",
        "\n",
        "    # Resolve conflicting classifications\n",
        "    def resolve_classification(group):\n",
        "        if 'cancerous' in group.dropna().values:\n",
        "            return 'cancerous'\n",
        "        return 'non-cancerous'\n",
        "\n",
        "    resolved_classifications = (\n",
        "        linked_data.groupby('submitter_id')[classification_column]\n",
        "        .apply(resolve_classification)\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # Merge resolved classifications back\n",
        "    linked_data = linked_data.drop(columns=[classification_column], errors='ignore').merge(\n",
        "        resolved_classifications, on='submitter_id', how='left'\n",
        "    )\n",
        "\n",
        "    # Drop remaining duplicates\n",
        "    linked_data = linked_data.drop_duplicates()\n",
        "\n",
        "# Display the cleaned data\n",
        "print(linked_data[['submitter_id', type_column, 'classification']].head())\n",
        "\n",
        "# Save cleaned data\n",
        "linked_data.to_csv(\"linked_rna_sequence_classification_cleaned.csv\", index=False)\n",
        "print(\"Cleaned data saved successfully!\")\n"
      ],
      "metadata": {
        "id": "uqKHVmQhq_CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resolve_classification(group):\n",
        "    if 'cancerous' in group['classification'].values:\n",
        "        return 'cancerous'\n",
        "    return 'non-cancerous'\n"
      ],
      "metadata": {
        "id": "_dU41sFarjp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resolve classification explicitly by type_new\n",
        "def resolve_classification(group):\n",
        "    if 'Primary Tumor' in group['type_new'].values or 'Metastatic' in group['type_new'].values:\n",
        "        return 'cancerous'\n",
        "    return 'non-cancerous'\n",
        "\n",
        "# Group by submitter_id and apply the resolve logic\n",
        "linked_data['classification'] = linked_data.groupby('submitter_id').apply(\n",
        "    lambda group: resolve_classification(group)\n",
        ").reset_index(level=0, drop=True)\n"
      ],
      "metadata": {
        "id": "YMgzBCa2scdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resolve classification explicitly by type_new\n",
        "def resolve_classification(group):\n",
        "    if 'Primary Tumor' in group['type_new'].values or 'Metastatic' in group['type_new'].values:\n",
        "        return 'cancerous'\n",
        "    return 'non-cancerous'\n",
        "\n",
        "# Group by submitter_id and apply the resolve logic\n",
        "linked_data['classification'] = linked_data.groupby('submitter_id', group_keys=False).apply(\n",
        "    lambda group: resolve_classification(group)\n",
        ")\n"
      ],
      "metadata": {
        "id": "KPahmNhWshzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resolve classification explicitly by type_new\n",
        "def resolve_classification(group):\n",
        "    if 'Primary Tumor' in group['type_new'].values or 'Metastatic' in group['type_new'].values:\n",
        "        return 'cancerous'\n",
        "    return 'non-cancerous'\n",
        "\n",
        "# Group by submitter_id and resolve classifications\n",
        "linked_data['classification'] = linked_data.groupby('submitter_id', group_keys=False).apply(\n",
        "    lambda group: pd.Series({'classification': resolve_classification(group)})\n",
        ").reset_index(drop=True)['classification']\n"
      ],
      "metadata": {
        "id": "eSRc6JrYtGcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resolve classification explicitly by type_new\n",
        "def resolve_classification(group):\n",
        "    if 'Primary Tumor' in group['type_new'].values or 'Metastatic' in group['type_new'].values:\n",
        "        return 'cancerous'\n",
        "    return 'non-cancerous'\n",
        "\n",
        "# Group by submitter_id, exclude grouping column, and resolve classifications\n",
        "linked_data['classification'] = (\n",
        "    linked_data.drop(columns=['classification'])  # Drop the existing classification column if it exists\n",
        "    .groupby('submitter_id', group_keys=False)\n",
        "    .apply(lambda group: resolve_classification(group))\n",
        ")\n",
        "\n",
        "# Ensure the new classification column aligns with the DataFrame\n",
        "linked_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(linked_data[['submitter_id', 'type_new', 'classification']].head())\n"
      ],
      "metadata": {
        "id": "gqzESQ3QtVnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the columns\n",
        "print(linked_data.columns)\n",
        "\n",
        "# Use 'type_new' as the classification column\n",
        "classification_column = 'type_new'  # Update this based on the correct column name\n",
        "\n",
        "# Ensure the classification column exists\n",
        "if classification_column not in linked_data.columns:\n",
        "    raise ValueError(f\"Column '{classification_column}' not found in the DataFrame.\")\n",
        "\n",
        "# Function to resolve classification\n",
        "def resolve_classification(group):\n",
        "    if group[classification_column].str.contains(\"Primary Tumor|Metastatic\", na=False).any():\n",
        "        return \"cancerous\"\n",
        "    elif group[classification_column].str.contains(\"Normal\", na=False).any():\n",
        "        return \"non-cancerous\"\n",
        "    return \"unknown\"\n",
        "\n",
        "# Apply classification resolution\n",
        "linked_data['classification'] = linked_data.groupby('submitter_id', group_keys=False).apply(\n",
        "    lambda group: pd.Series(resolve_classification(group), index=group.index)\n",
        ")\n",
        "\n",
        "# Save and inspect the cleaned data\n",
        "linked_data.to_csv(\"linked_rna_sequence_classification_cleaned.csv\", index=False)\n",
        "print(linked_data[['submitter_id', classification_column, 'classification']].drop_duplicates().head())\n",
        "print(\"Cleaned data saved successfully!\")\n"
      ],
      "metadata": {
        "id": "12xd8llVvTfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the columns in the DataFrame\n",
        "print(linked_data.columns)\n",
        "\n",
        "# Ensure the 'type_new' column exists\n",
        "classification_column = 'type_new'\n",
        "if classification_column not in linked_data.columns:\n",
        "    raise ValueError(f\"Column '{classification_column}' not found in the DataFrame.\")\n",
        "\n",
        "# Define classification logic based on 'type_new', handling NaN values\n",
        "def classify(row):\n",
        "    type_value = row[classification_column]\n",
        "    if isinstance(type_value, str) and \"Normal\" in type_value:\n",
        "        return \"non-cancerous\"\n",
        "    elif isinstance(type_value, str):\n",
        "        return \"cancerous\"\n",
        "    return \"unknown\"  # Default classification for NaN or other unexpected values\n",
        "\n",
        "# Apply classification logic to each row\n",
        "linked_data['classification'] = linked_data.apply(classify, axis=1)\n",
        "\n",
        "# Save and inspect the cleaned data\n",
        "linked_data.to_csv(\"linked_rna_sequence_classification_cleaned.csv\", index=False)\n",
        "print(linked_data[['submitter_id', classification_column, 'classification']].drop_duplicates().head())\n",
        "print(\"Cleaned data saved successfully!\")\n"
      ],
      "metadata": {
        "id": "F7E2HLW7wTd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation Code"
      ],
      "metadata": {
        "id": "SZqu5VYXxkUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the classification rule is correctly applied across the DataFrame\n",
        "incorrect_classifications = linked_data[\n",
        "    ((linked_data['type_new'].str.contains(\"Normal\", na=False)) & (linked_data['classification'] != \"non-cancerous\")) |\n",
        "    ((~linked_data['type_new'].str.contains(\"Normal\", na=False)) & (linked_data['classification'] != \"cancerous\"))\n",
        "]\n",
        "\n",
        "# Display rows with incorrect classifications\n",
        "if not incorrect_classifications.empty:\n",
        "    print(\"Incorrect classifications found:\")\n",
        "    print(incorrect_classifications)\n",
        "else:\n",
        "    print(\"All rows are correctly classified based on the rule.\")\n",
        "\n",
        "# Display a sample of the cleaned DataFrame for visual inspection\n",
        "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Linked RNA Sequence Classification\", dataframe=linked_data)\n",
        "\n",
        "# Optional: Save the DataFrame again for reference\n",
        "linked_data.to_csv(\"final_linked_rna_sequence_classification.csv\", index=False)\n",
        "print(\"Final cleaned and validated data saved successfully!\")\n"
      ],
      "metadata": {
        "id": "u7qaZ0dwwk29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ace-tools"
      ],
      "metadata": {
        "id": "kEcGJAxqyb2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter rows with missing type_new values\n",
        "missing_type_new = linked_data[linked_data['type_new'].isna()]\n",
        "print(f\"Rows with missing 'type_new': {len(missing_type_new)}\")\n",
        "\n",
        "# Optionally inspect the rows with missing 'type_new'\n",
        "print(missing_type_new[['submitter_id', 'type_new', 'classification']].head())\n",
        "\n",
        "# Apply classification only to rows with valid 'type_new'\n",
        "def classify(row):\n",
        "    if pd.notna(row['type_new']) and \"Normal\" in row['type_new']:\n",
        "        return \"non-cancerous\"\n",
        "    elif pd.notna(row['type_new']):\n",
        "        return \"cancerous\"\n",
        "    return \"unknown\"  # Handle cases where type_new is NaN\n",
        "\n",
        "linked_data['classification'] = linked_data.apply(classify, axis=1)\n",
        "\n",
        "# Check for any remaining unknown classifications\n",
        "unknown_classifications = linked_data[linked_data['classification'] == \"unknown\"]\n",
        "print(f\"Rows with 'unknown' classification after applying logic: {len(unknown_classifications)}\")\n",
        "\n",
        "# Save the final DataFrame\n",
        "linked_data.to_csv(\"final_linked_rna_sequence_classification_cleaned.csv\", index=False)\n",
        "print(\"Final cleaned and validated data saved successfully!\")\n",
        "\n",
        "# Display the final DataFrame\n",
        "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Linked RNA Sequence Classification Cleaned\", dataframe=linked_data)\n"
      ],
      "metadata": {
        "id": "fUL-6Z19xpyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming the DataFrame `linked_data` is already loaded or created earlier\n",
        "\n",
        "# Filter rows with missing type_new values\n",
        "missing_type_new = linked_data[linked_data['type_new'].isna()]\n",
        "print(f\"Rows with missing 'type_new': {len(missing_type_new)}\")\n",
        "\n",
        "# Apply classification only to rows where type_new is present\n",
        "def classify(row):\n",
        "    if pd.notna(row['type_new']) and \"Normal\" in row['type_new']:\n",
        "        return \"non-cancerous\"\n",
        "    elif pd.notna(row['type_new']):\n",
        "        return \"cancerous\"\n",
        "    return \"unknown\"  # Assign \"unknown\" for rows with NaN in 'type_new'\n",
        "\n",
        "linked_data['classification'] = linked_data.apply(classify, axis=1)\n",
        "\n",
        "# Check for any remaining unknown classifications\n",
        "unknown_classifications = linked_data[linked_data['classification'] == \"unknown\"]\n",
        "print(f\"Rows with 'unknown' classification after applying logic: {len(unknown_classifications)}\")\n",
        "\n",
        "# Save the final cleaned data\n",
        "linked_data.to_csv(\"final_linked_rna_sequence_classification_cleaned.csv\", index=False)\n",
        "print(\"Final cleaned and validated data saved successfully!\")\n",
        "\n",
        "# Display the DataFrame to the user for review\n",
        "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Linked RNA Sequence Classification Cleaned\", dataframe=linked_data)\n"
      ],
      "metadata": {
        "id": "_1spLVvTyXP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Save the cleaned DataFrame for download\n",
        "file_path = \"Linked_RNA_Sequence_Classification_Cleaned.csv\"\n",
        "linked_data.to_csv(file_path, index=False)\n",
        "\n",
        "# Notify the user the file is ready for download\n",
        "file_path\n"
      ],
      "metadata": {
        "id": "remX0dDqzC1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the cleaned DataFrame locally in a robust way\n",
        "local_file_path = 'Linked_RNA_Sequence_Classification_Cleaned.csv'\n",
        "linked_data.to_csv(local_file_path, index=False)\n",
        "\n",
        "# Provide the path to the user\n",
        "local_file_path\n"
      ],
      "metadata": {
        "id": "07OG9eYiziwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "BDlYGIbmzqY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ltr"
      ],
      "metadata": {
        "id": "I8TF10t10HdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "WqgPLgQd0Khp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "check for duplicates"
      ],
      "metadata": {
        "id": "nVndG5km3NZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the provided CSV file\n",
        "file_path = 'linked_rna_sequence_classification_cleaned.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Check for duplicates in the dataset\n",
        "duplicates = data[data.duplicated()]\n",
        "\n",
        "# Display information about duplicates\n",
        "duplicates_count = duplicates.shape[0]\n",
        "\n",
        "duplicates_count\n"
      ],
      "metadata": {
        "id": "86gsfR-20Rzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### dynamically reading the file_id column from the CSV file and fetching the RNA sequences using the GDC API would be the most efficient and dynamic approach. We can then write the fetched RNA sequences into a new column named sequence in the same DataFrame and save the updated CSV.\n",
        "\n",
        "#### Steps:\n",
        "1. Load the CSV file: Read the CSV file to extract file_ids dynamically.\n",
        "\n",
        "2. Query the GDC API: Use the GDC API to fetch RNA sequence data for each file_id.\n",
        "\n",
        "3. Add Sequences to the DataFrame: Store the RNA sequences in a new column sequence.\n",
        "\n",
        "4. Save the Updated Data: Save the DataFrame with the new column to a new CSV file."
      ],
      "metadata": {
        "id": "1dXiSjTT8u8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# Path to the cleaned CSV file\n",
        "csv_file_path = \"/content/gdc-client/bin/linked_rna_sequence_classification_cleaned_cg_edited.csv\"\n",
        "\n",
        "# Load the CSV file\n",
        "data = pd.read_csv(csv_file_path)\n",
        "\n",
        "# GDC API URL\n",
        "BASE_URL = \"https://api.gdc.cancer.gov/data/\"\n",
        "\n",
        "# Function to fetch RNA sequence data for a given file_id\n",
        "def fetch_rna_sequence(file_id):\n",
        "    try:\n",
        "        url = f\"{BASE_URL}{file_id}\"\n",
        "        response = requests.get(url, stream=True)\n",
        "        if response.status_code == 200:\n",
        "            # Assuming response contains plain text RNA sequence data\n",
        "            return response.text[:100]  # Keep first 100 characters for brevity (adjust as needed)\n",
        "        else:\n",
        "            print(f\"Failed to fetch data for file_id {file_id}: {response.status_code}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data for file_id {file_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Add a new column to store RNA sequences\n",
        "data['sequence'] = data['file_id'].apply(fetch_rna_sequence)\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_csv_path = \"/content/linked_rna_sequences_with_data.csv\"\n",
        "data.to_csv(output_csv_path, index=False)\n",
        "\n",
        "print(f\"Updated CSV with RNA sequences saved to: {output_csv_path}\")\n"
      ],
      "metadata": {
        "id": "C4aGL8xo3XvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Here is the full Python script for extracting FASTQ sequences using file IDs from the cleaned dataset and writing the corresponding data to a CSV file.\n",
        "\n",
        "#### This script assumes:\n",
        "\n",
        "1. We are querying the GDC API for the FASTQ sequences.\n",
        "2. The cleaned dataset is already prepared and available in CSV format.\n",
        "3. The script dynamically retrieves the FASTQ sequences and appends them to the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "FnjoJ8OrMPeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Input and output file paths\n",
        "input_csv = \"/content/drive/MyDrive/Colab Notebooks/RNA_GDC_Labellled/linked_rna_sequence_classification_cleaned_cg_edited.csv\"\n",
        "output_csv = \"/content/linked_rna_sequences_with_fastq.csv\"\n",
        "gdc_api_url = \"https://api.gdc.cancer.gov/data\"\n",
        "\n",
        "# Function to download FASTQ sequences for a given file ID\n",
        "def download_fastq(file_id):\n",
        "    try:\n",
        "        response = requests.get(f\"{gdc_api_url}/{file_id}\", stream=True)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Save the FASTQ file locally for verification (optional)\n",
        "        file_path = f\"/content/fastq_files/{file_id}.fastq\"\n",
        "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "        with open(file_path, \"wb\") as f:\n",
        "            for chunk in response.iter_content(chunk_size=1024):\n",
        "                f.write(chunk)\n",
        "\n",
        "        return file_path  # Return the saved file path\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading FASTQ for file ID {file_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load the cleaned dataset\n",
        "df = pd.read_csv(input_csv)\n",
        "\n",
        "# Ensure a directory exists for storing FASTQ files\n",
        "os.makedirs(\"/content/fastq_files\", exist_ok=True)\n",
        "\n",
        "# Initialize a new column for FASTQ file paths\n",
        "df[\"fastq_path\"] = None\n",
        "\n",
        "# Iterate through the dataset and retrieve FASTQ sequences\n",
        "for index, row in df.iterrows():\n",
        "    file_id = row[\"file_id\"]\n",
        "    print(f\"Processing file ID: {file_id}\")\n",
        "\n",
        "    # Download the FASTQ file and store the path\n",
        "    fastq_path = download_fastq(file_id)\n",
        "    if fastq_path:\n",
        "        df.at[index, \"fastq_path\"] = fastq_path\n",
        "\n",
        "# Save the updated dataset with FASTQ file paths\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"FASTQ sequences extraction complete. Results saved to: {output_csv}\")\n"
      ],
      "metadata": {
        "id": "A2KlYGFq9Q1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2F_aHVDeMw9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install biopython"
      ],
      "metadata": {
        "id": "N6QYDhBMMtBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from Bio import SeqIO\n",
        "import os\n",
        "\n",
        "# Input CSV file path\n",
        "input_csv = \"/content/drive/MyDrive/Colab Notebooks/RNA_GDC_Labellled/linked_rna_sequence_classification_cleaned_cg_edited.csv\"\n",
        "\n",
        "# Output CSV file path\n",
        "output_csv = \"/content/drive/MyDrive/Colab Notebooks/RNA_GDC_Labellled/linked_rna_sequence_classification_with_sequences.csv\"\n",
        "\n",
        "# Read the CSV file\n",
        "data = pd.read_csv(input_csv)\n",
        "\n",
        "# Ensure there is a column for FASTQ sequences\n",
        "data['fastq_seq'] = \"\"\n",
        "\n",
        "# Iterate over each row to process the FASTQ files\n",
        "for index, row in data.iterrows():\n",
        "    # Assuming 'file_id' column exists and FASTQ files are named using the file_id\n",
        "    fastq_path = f\"/content/fastq_files/{row['file_id']}.fastq\"\n",
        "\n",
        "    if os.path.exists(fastq_path):\n",
        "        sequences = []\n",
        "        # Parse the FASTQ file and extract sequences\n",
        "        with open(fastq_path, \"r\") as handle:\n",
        "            for record in SeqIO.parse(handle, \"fastq\"):\n",
        "                sequences.append(str(record.seq))  # Extract sequence\n",
        "\n",
        "        # Combine sequences into a single string\n",
        "        data.at[index, 'fastq_seq'] = \";\".join(sequences)  # Combine all sequences\n",
        "    else:\n",
        "        # Mark the sequence as missing if the file doesn't exist\n",
        "        data.at[index, 'fastq_seq'] = \"MISSING\"\n",
        "\n",
        "# Save the updated DataFrame back to a CSV file\n",
        "data.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"Updated CSV with sequences saved to: {output_csv}\")\n"
      ],
      "metadata": {
        "id": "1ym3GKMISGZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.head())  # Inspect the first few rows to ensure the DataFrame is as expected\n",
        "print(data.columns)  # Verify column names\n"
      ],
      "metadata": {
        "id": "x54BzlAcTFDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv(output_csv, index=False)\n",
        "print(f\"CSV file successfully saved to {output_csv}\")\n"
      ],
      "metadata": {
        "id": "eF2FdcLbW4Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.path.exists(output_csv))  # Should return True if the file was saved\n"
      ],
      "metadata": {
        "id": "sIU8t3WIXKBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_valid_fastq(file_path):\n",
        "    try:\n",
        "        with open(file_path, \"r\") as handle:\n",
        "            for record in SeqIO.parse(handle, \"fastq\"):\n",
        "                return True  # File is valid if parsing succeeds\n",
        "    except Exception:\n",
        "        return False  # File is invalid if an exception occurs\n"
      ],
      "metadata": {
        "id": "FzVvyInJXYu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.columns)\n"
      ],
      "metadata": {
        "id": "18_bypLNgoXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the base directory where FASTQ files are stored\n",
        "fastq_base_dir = \"/path/to/fastq_files\"  # Replace with the actual path to your FASTQ files\n",
        "\n",
        "# Add a new column for the constructed FASTQ file paths\n",
        "data['fastq_path'] = data['file_id'].apply(lambda x: os.path.join(fastq_base_dir, f\"{x}.fastq\"))\n",
        "\n",
        "# Debugging: Verify the new column\n",
        "print(data[['file_id', 'fastq_path']].head())\n"
      ],
      "metadata": {
        "id": "M7DYfqEsgvgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fastq_path = \"/content/fastq_files\"  # Replace with the correct directory /content/fastq_files\n"
      ],
      "metadata": {
        "id": "O-hM_LemhPFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the actual directory of FASTQ files\n",
        "fastq_base_dir = \"/content/fastq_files\"  # Update with the correct path\n",
        "\n",
        "# Create the `fastq_path` column\n",
        "data['fastq_path'] = data['file_id'].apply(lambda x: os.path.join(fastq_base_dir, f\"{x}.fastq\"))\n",
        "\n",
        "# Check for missing files\n",
        "missing_files = data[~data['fastq_path'].apply(os.path.exists)]\n",
        "if not missing_files.empty:\n",
        "    print(\"Missing files detected:\")\n",
        "    print(missing_files[['file_id', 'fastq_path']])\n",
        "else:\n",
        "    print(\"All files found.\")\n"
      ],
      "metadata": {
        "id": "khZJBWmyiwL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: If the column name is 'file_path'\n",
        "for index, row in data.iterrows():\n",
        "    fastq_path = row['fastq_path']  # Replace 'file_path' with the actual column name\n",
        "    try:\n",
        "        sequences = []\n",
        "        with open(fastq_path, \"r\") as handle:\n",
        "            for record in SeqIO.parse(handle, \"fastq\"):\n",
        "                sequences.append(str(record.seq))\n",
        "        sequences_column.append(\";\".join(sequences))\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {fastq_path}: {e}\")\n",
        "        sequences_column.append(\"ERROR\")\n"
      ],
      "metadata": {
        "id": "Z73h5BEck0qY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.columns)  # Lists all column names\n",
        "print(data.head())   # Displays the first few rows\n"
      ],
      "metadata": {
        "id": "ijpDq1g7lO9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from Bio import SeqIO\n",
        "import os\n",
        "\n",
        "# Input CSV file path\n",
        "input_csv = \"/content/drive/MyDrive/Colab Notebooks/RNA_GDC_Labellled/linked_rna_sequence_classification_cleaned_cg_edited.csv\"  # Update this with your actual file path\n",
        "\n",
        "# Output CSV file path\n",
        "output_csv = \"/content/drive/MyDrive/Colab Notebooks/RNA_GDC_Labellled/linked_rna_sequence_classification_with_sequences_we_hope.csv\"  # Update this with your desired output path\n",
        "\n",
        "# Read the CSV file\n",
        "data = pd.read_csv(input_csv)\n",
        "\n",
        "# Initialize a list to hold the sequences\n",
        "sequences_column = []\n",
        "\n",
        "# Function to validate FASTQ file format\n",
        "def is_valid_fastq(filepath):\n",
        "    try:\n",
        "        with open(filepath, \"r\") as handle:\n",
        "            for _ in SeqIO.parse(handle, \"fastq\"):\n",
        "                return True  # If we can parse at least one record, it's valid\n",
        "    except Exception as e:\n",
        "        print(f\"Error validating FASTQ file {filepath}: {e}\")\n",
        "    return False\n",
        "\n",
        "# Iterate over each row in the DataFrame\n",
        "for index, row in data.iterrows():\n",
        "    fastq_path = row['fastq_path']  # Ensure this column contains the FASTQ file paths\n",
        "\n",
        "    # Check if the file exists\n",
        "    if not os.path.exists(fastq_path):\n",
        "        print(f\"Missing file: {fastq_path}\")\n",
        "        sequences_column.append(\"MISSING\")\n",
        "        continue\n",
        "\n",
        "    # Check if the file is a valid FASTQ\n",
        "    if not is_valid_fastq(fastq_path):\n",
        "        print(f\"Invalid FASTQ file: {fastq_path}\")\n",
        "        sequences_column.append(\"INVALID\")\n",
        "        continue\n",
        "\n",
        "    # Extract sequences from the FASTQ file\n",
        "    sequences = []\n",
        "    try:\n",
        "        with open(fastq_path, \"r\") as handle:\n",
        "            for record in SeqIO.parse(handle, \"fastq\"):\n",
        "                sequences.append(str(record.seq))  # Extract sequence\n",
        "        sequences_column.append(\";\".join(sequences))  # Combine all sequences\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {fastq_path}: {e}\")\n",
        "        sequences_column.append(\"ERROR\")\n",
        "\n",
        "# Add the sequences as a new column\n",
        "data['fastq_seq'] = sequences_column\n",
        "\n",
        "# Save the updated DataFrame back to a CSV file\n",
        "data.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"Updated CSV with sequences saved to: {output_csv}\")\n"
      ],
      "metadata": {
        "id": "nW9mzQXhlXO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([col for col in data.columns])\n"
      ],
      "metadata": {
        "id": "Jol0tSJ6mmZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(input_csv, dtype=str)  # Read all columns as strings to avoid unexpected behavior\n",
        "print(data.columns)  # Confirm the column names\n"
      ],
      "metadata": {
        "id": "DSsRBmBnmHbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['fastq_path'] = \"/path/to/fastq_files/\" + data['file_id'] + \".fastq\"\n"
      ],
      "metadata": {
        "id": "nQhwVr7Ym7hJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory containing FASTQ files\n",
        "fastq_dir = \"/content/fastq_files/\"  # Update to your actual directory path\n",
        "\n",
        "# Generate the full paths to the FASTQ files\n",
        "data['fastq_path'] = fastq_dir + data['file_id'] + \".fastq\"\n"
      ],
      "metadata": {
        "id": "PZ9r33NMnWiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from Bio import SeqIO\n",
        "import os\n",
        "\n",
        "# Input CSV file path\n",
        "input_csv = \"/content/drive/MyDrive/Colab Notebooks/RNA_GDC_Labellled/linked_rna_sequence_classification_cleaned_cg_edited.csv\"\n",
        "\n",
        "# Output CSV file path\n",
        "output_csv = \"/content/drive/MyDrive/Colab Notebooks/RNA_GDC_Labellled/linked_rna_sequence_classification_with_sequences.csv\"\n",
        "\n",
        "# FASTQ file directory (update this to your actual directory)\n",
        "fastq_dir = \"/content/fastq_files/\"\n",
        "\n",
        "# Read the CSV file\n",
        "data = pd.read_csv(input_csv)\n",
        "\n",
        "# Add the fastq_path column if not already present\n",
        "if 'fastq_path' not in data.columns:\n",
        "    data['fastq_path'] = fastq_dir + data['file_id'] + \".fastq\"\n",
        "\n",
        "# Initialize a list to hold the sequences\n",
        "sequences_column = []\n",
        "\n",
        "# Function to validate FASTQ file format\n",
        "def is_valid_fastq(filepath):\n",
        "    try:\n",
        "        with open(filepath, \"r\") as handle:\n",
        "            for _ in SeqIO.parse(handle, \"fastq\"):\n",
        "                return True  # If we can parse at least one record, it's valid\n",
        "    except Exception as e:\n",
        "        print(f\"Error validating FASTQ file {filepath}: {e}\")\n",
        "    return False\n",
        "\n",
        "# Iterate over each row in the DataFrame\n",
        "for index, row in data.iterrows():\n",
        "    fastq_path = row['fastq_path']  # Column containing the FASTQ file paths\n",
        "\n",
        "    # Check if the file exists\n",
        "    if not os.path.exists(fastq_path):\n",
        "        print(f\"Missing file: {fastq_path}\")\n",
        "        sequences_column.append(\"MISSING\")\n",
        "        continue\n",
        "\n",
        "    # Check if the file is a valid FASTQ\n",
        "    if not is_valid_fastq(fastq_path):\n",
        "        print(f\"Invalid FASTQ file: {fastq_path}\")\n",
        "        sequences_column.append(\"INVALID\")\n",
        "        continue\n",
        "\n",
        "    # Extract sequences from the FASTQ file\n",
        "    sequences = []\n",
        "    try:\n",
        "        with open(fastq_path, \"r\") as handle:\n",
        "            for record in SeqIO.parse(handle, \"fastq\"):\n",
        "                sequences.append(str(record.seq))  # Extract sequence\n",
        "        sequences_column.append(\";\".join(sequences))  # Combine all sequences\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {fastq_path}: {e}\")\n",
        "        sequences_column.append(\"ERROR\")\n",
        "\n",
        "# Add the sequences as a new column\n",
        "data['fastq_seq'] = sequences_column\n",
        "\n",
        "# Save the updated DataFrame back to a CSV file\n",
        "data.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"Updated CSV with sequences saved to: {output_csv}\")\n"
      ],
      "metadata": {
        "id": "hiGZSA-knarE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv(\"/content/updated_file_with_fastq_paths.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "ALDFYGgBnx5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def validate_fastq(file_path):\n",
        "    \"\"\"\n",
        "    Validates the FASTQ file to ensure each record starts with '@'.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for i in range(0, len(lines), 4):  # FASTQ format: 4 lines per record\n",
        "            if not lines[i].startswith('@'):\n",
        "                return False\n",
        "    return True\n",
        "\n",
        "def fix_fastq(file_path, output_path):\n",
        "    \"\"\"\n",
        "    Adds '@' to the start of records missing the '@' character.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    fixed_lines = []\n",
        "    for i in range(0, len(lines), 4):\n",
        "        if not lines[i].startswith('@'):\n",
        "            lines[i] = '@' + lines[i].strip() + '\\n'\n",
        "        fixed_lines.extend(lines[i:i+4])\n",
        "\n",
        "    with open(output_path, 'w') as output_file:\n",
        "        output_file.writelines(fixed_lines)\n",
        "\n",
        "# Path to the directory containing FASTQ files\n",
        "directory_path = \"/content/fastq_files\"\n",
        "output_directory = \"/content/validated_fastq_files\"\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "for filename in os.listdir(directory_path):\n",
        "    file_path = os.path.join(directory_path, filename)\n",
        "    output_path = os.path.join(output_directory, filename)\n",
        "\n",
        "    if validate_fastq(file_path):\n",
        "        print(f\"{filename} is valid.\")\n",
        "    else:\n",
        "        print(f\"{filename} is invalid. Attempting to fix...\")\n",
        "        fix_fastq(file_path, output_path)\n",
        "        if validate_fastq(output_path):\n",
        "            print(f\"{filename} was successfully fixed and saved to {output_path}.\")\n",
        "        else:\n",
        "            print(f\"Failed to fix {filename}. Please inspect the file manually.\")\n"
      ],
      "metadata": {
        "id": "jJ9vXnTsoMOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# File path to the CSV\n",
        "csv_file_path = '/content/drive/MyDrive/RNA_sequence_files/linked_rna_sequence_classification_cleaned_cg_edited.csv'\n",
        "\n",
        "# Step 1: Read the CSV file and extract 'file_id'\n",
        "df = pd.read_csv(csv_file_path)\n",
        "file_ids = df['file_id'].tolist()  # Assuming 'file_id' is the column name\n",
        "\n",
        "# Step 2: GraphQL API endpoint and headers\n",
        "graphql_url = \"https://api.example.com/graphql\"  # Replace with actual endpoint\n",
        "headers = {\n",
        "    \"Authorization\": \"Bearer YOUR_API_TOKEN\",  # Replace with your token\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# Step 3: Construct GraphQL query dynamically for each file_id\n",
        "for file_id in file_ids:\n",
        "    # GraphQL query with dynamic file_id\n",
        "    query = f\"\"\"\n",
        "    {{\n",
        "      submitted_unaligned_reads (file_id: \"{file_id}\") {{\n",
        "        file_name\n",
        "        id\n",
        "        project_id\n",
        "        submitter_id\n",
        "      }}\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 4: Send the POST request to GraphQL endpoint\n",
        "    response = requests.post(graphql_url, json={\"query\": query}, headers=headers)\n",
        "\n",
        "    # Step 5: Process the response\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        # Extract and display the results\n",
        "        if 'data' in data and 'submitted_unaligned_reads' in data['data']:\n",
        "            for item in data['data']['submitted_unaligned_reads']:\n",
        "                print(f\"File ID: {file_id}, File Name: {item['file_name']}, UUID: {item['id']}, Project ID: {item['project_id']}, Submitter ID: {item['submitter_id']}\")\n",
        "        else:\n",
        "            print(f\"No data found for File ID: {file_id}\")\n",
        "    else:\n",
        "        print(f\"Error querying File ID {file_id}: {response.status_code}, {response.text}\")\n"
      ],
      "metadata": {
        "id": "hdoyt1zOolc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graphql_url = \"https://actual-api-endpoint.com/graphql\"\n"
      ],
      "metadata": {
        "id": "kHrkvY25ry7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://actual-api-endpoint.com/graphql\n"
      ],
      "metadata": {
        "id": "pOd3MiA8sjPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import subprocess\n",
        "\n",
        "# Define the CSV file path and download directory\n",
        "csv_file_path = \"/content/drive/MyDrive/RNA_sequence_files/linked_rna_sequence_classification_cleaned_cg_edited.csv\"\n",
        "download_dir = \"/content/drive/MyDrive/RNA_sequence_files\"  # Replace with your desired download directory\n",
        "\n",
        "# Ensure the download directory exists\n",
        "os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "# Function to read file IDs from the CSV file\n",
        "def get_file_ids_from_csv(csv_path):\n",
        "    file_ids = []\n",
        "    try:\n",
        "        with open(csv_path, mode=\"r\") as csv_file:\n",
        "            reader = csv.DictReader(csv_file)\n",
        "            # Assuming the column name for file_id is \"file_id\"\n",
        "            for row in reader:\n",
        "                if \"file_id\" in row and row[\"file_id\"].strip():\n",
        "                    file_ids.append(row[\"file_id\"].strip())\n",
        "        print(f\"Extracted {len(file_ids)} file IDs from the CSV.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV file: {e}\")\n",
        "    return file_ids\n",
        "\n",
        "# Function to download a single file without a token\n",
        "def download_file(file_id):\n",
        "    command = [\n",
        "        \"gdc-client\", \"download\",\n",
        "        \"-d\", download_dir,\n",
        "        \"-f\", file_id\n",
        "    ]\n",
        "    try:\n",
        "        subprocess.run(command, check=True)\n",
        "        print(f\"Successfully downloaded file with ID: {file_id}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Failed to download file with ID: {file_id}\")\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "# Main execution\n",
        "file_ids = get_file_ids_from_csv(csv_file_path)\n",
        "\n",
        "if file_ids:\n",
        "    for file_id in file_ids:\n",
        "        download_file(file_id)\n",
        "    print(\"All downloads completed.\")\n",
        "else:\n",
        "    print(\"No file IDs found. Please check the CSV file.\")\n"
      ],
      "metadata": {
        "id": "ywU9-foqspgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OsojjXkT1Eek"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}